# search_disease.py
import os
import django

# --- Khá»Ÿi táº¡o Django ---
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'KLTN.settings')  # Ä‘á»•i 'KLTN' thÃ nh tÃªn project cá»§a báº¡n
django.setup()

# --- Import model sau khi setup ---
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from ICD10.models.icd10 import ICDDisease, DiseaseExtraInfo

# --- Load model ngÃ´n ngá»¯ ---
# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
model = SentenceTransformer("intfloat/multilingual-e5-base")

# --- Dá»¯ liá»‡u ---
texts = []
codes = []
diseases = ICDDisease.objects.all()
for d in diseases:
    extra_info = DiseaseExtraInfo.objects.filter(disease=d).first()
    if extra_info:
        texts.append(f"{d.code} - {d.title_vi} - {extra_info.description or ''} - {extra_info.symptoms or ''}")
    else:
        texts.append(f"{d.code} - {d.title_vi}")
    codes.append(d.code)
    

print(f"âœ… ÄÃ£ táº£i {len(texts)} bá»‡nh ICD10.")

batch_size = 64
embeddings = []

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i + batch_size]
    batch_embeddings = model.encode(
        [f"query: {t}" for t in batch_texts],
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    embeddings.append(batch_embeddings)

    # Log tiáº¿n trÃ¬nh má»—i 500 báº£n ghi
    if i % 500 == 0:
        print(f"âœ… ÄÃ£ encode {i}/{len(texts)} bá»‡nh...")

# Gá»™p láº¡i
embeddings = np.vstack(embeddings)
print(f"âœ… HoÃ n táº¥t encode {len(embeddings)} embeddings, kÃ­ch thÆ°á»›c {embeddings.shape[1]}.")

# ============================================================
# ğŸ’¾ 3. Táº¡o FAISS Index (cosine similarity)
# ============================================================

dimension = embeddings.shape[1]

# ğŸ§  Option A: Dáº¡ng full precision (chÃ­nh xÃ¡c nháº¥t, tá»‘n RAM hÆ¡n)
index = faiss.IndexFlatIP(dimension)

# ğŸ§  Option B: Dáº¡ng nÃ©n IVF (giáº£m RAM, tá»‘c Ä‘á»™ nhanh hÆ¡n khi dá»¯ liá»‡u lá»›n)
# quantizer = faiss.IndexFlatIP(dimension)
# index = faiss.IndexIVFFlat(quantizer, dimension, 100)
# index.train(embeddings)

index.add(embeddings)

# ============================================================
# ğŸ“¦ 4. LÆ°u index vÃ  dá»¯ liá»‡u há»— trá»£
# ============================================================
faiss.write_index(index, "icd10_index_vi.faiss")
np.save("icd10_texts_vi.npy", np.array(texts, dtype=object))
np.save("icd10_codes.npy", np.array(codes, dtype=object))

print("ğŸ‰ ÄÃ£ lÆ°u xong:")
print("   â”œâ”€â”€ icd10_index_vi.faiss")
print("   â”œâ”€â”€ icd10_texts_vi.npy")
print("   â””â”€â”€ icd10_codes.npy")

# ============================================================
# ğŸ§ª 5. Kiá»ƒm tra thá»­
# ============================================================
index = faiss.read_index("icd10_index_vi.faiss")
texts = np.load("icd10_texts_vi.npy", allow_pickle=True)
codes = np.load("icd10_codes.npy", allow_pickle=True)

print(f"ğŸ§ª Kiá»ƒm tra láº¡i: cÃ³ {len(codes)} bá»‡nh, index chá»©a {index.ntotal} vectors.")

# VÃ­ dá»¥ truy váº¥n thá»­
query = "ho, sá»‘t, Ä‘au Ä‘áº§u"
query_emb = model.encode([f"query: {query}"], convert_to_numpy=True, normalize_embeddings=True)
scores, idxs = index.search(query_emb, 5)

print("\nğŸ” Káº¿t quáº£ tÃ¬m kiáº¿m gáº§n nháº¥t:")
for rank, i in enumerate(idxs[0]):
    print(f"#{rank+1}: {codes[i]} | {texts[i][:120]}...")

